{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Nv6UldrVuXv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\khaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\khaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import operator\n",
    "import requests\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import operator\n",
    "import requests\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords') # If needed\n",
    "nltk.download('punkt') # If needed\n",
    "nltk.download('wordnet') # If needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vR3p8oif2EC6"
   },
   "outputs": [],
   "source": [
    "#Positive and negative reviews\n",
    "\n",
    "#train data\n",
    "path_train_pos='imdb_train_pos.txt'\n",
    "train_pos=open(path_train_pos,encoding='utf8').readlines()\n",
    "path_train_neg='imdb_train_neg.txt'\n",
    "train_neg=open(path_train_neg,encoding='utf8').readlines()\n",
    "\n",
    "#test data\n",
    "path_test_pos='imdb_test_pos.txt'\n",
    "test_pos=open(path_test_pos,encoding='utf8').readlines()\n",
    "path_test_neg='imdb_test_neg.txt'\n",
    "test_neg=open(path_test_neg,encoding='utf8').readlines()\n",
    "\n",
    "#dev data\n",
    "path_dev_pos='imdb_dev_pos.txt'\n",
    "dev_pos=open(path_dev_pos,encoding='utf8').readlines()\n",
    "path_dev_neg='imdb_dev_neg.txt'\n",
    "dev_neg=open(path_dev_neg,encoding='utf8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "for pos_review in train_pos:\n",
    "    train.append((pos_review,1))\n",
    "for neg_review in train_neg:\n",
    "    train.append((neg_review,0))\n",
    "\n",
    "\n",
    "test=[]\n",
    "for pos_review in test_pos:\n",
    "    test.append((pos_review,1))\n",
    "for neg_review in test_neg:\n",
    "    test.append((neg_review,0))\n",
    "\n",
    "\n",
    "dev=[]\n",
    "for pos_review in dev_pos:\n",
    "    dev.append((pos_review,1))\n",
    "for neg_review in dev_neg:\n",
    "    dev.append((neg_review,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29802\n",
      "[('good', 8100), ('great', 5386), ('bad', 5340), ('many', 4127), ('much', 4088), ('little', 3368), ('first', 2927), ('real', 2718), ('old', 2334), ('new', 2268), ('funny', 2089), ('big', 2028), ('young', 1951), ('whole', 1748), ('br', 1745), ('original', 1706), ('ive', 1683), ('last', 1680), ('u', 1543), ('main', 1414), ('different', 1349), ('sure', 1310), ('american', 1289), ('special', 1286), ('dont', 1275), ('true', 1239), ('black', 1232), ('hard', 1208), ('second', 1169), ('high', 1159), ('short', 1151), ('poor', 1146), ('cant', 1104), ('classic', 1076), ('give', 1066), ('excellent', 1039), ('beautiful', 1029), ('right', 1028), ('full', 1018), ('human', 978), ('nice', 972), ('stupid', 965), ('interesting', 960), ('dead', 941), ('terrible', 925), ('wrong', 920), ('im', 918), ('enough', 900), ('long', 890), ('small', 888)]\n"
     ]
    }
   ],
   "source": [
    "    def clean_doc(doc):\n",
    "    # Split string into list\n",
    "    tokens = doc.split()\n",
    "    # Get rid of unwanted punctuation\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #Use isalpha() to remove words that don't begin with a letter from the alphabet\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Stop_words removes mispelled words, punctuation and symbols\n",
    "    stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "    stop_words.add(\".\")\n",
    "    stop_words.add(\",\")\n",
    "    stop_words.add(\"--\")\n",
    "    stop_words.add(\"``\")\n",
    "    stop_words.add(\"#\")\n",
    "    stop_words.add(\"@\")\n",
    "    stop_words.add(\":\")\n",
    "    stop_words.add(\"'s\")\n",
    "    stop_words.add(\"â€™\")\n",
    "    stop_words.add(\"...\")\n",
    "    stop_words.add(\"n't\")\n",
    "    stop_words.add(\"'\")\n",
    "    stop_words.add(\"-\")\n",
    "    stop_words.add(\";\")\n",
    "    stop_words.add(\"http\")\n",
    "    stop_words.add(\"!\")\n",
    "    stop_words.add(\"?\")\n",
    "    stop_words.add(\"&\")\n",
    "    stop_words.add(\")\")\n",
    "    stop_words.add('wa')\n",
    "    stop_words.add('u')\n",
    "    stop_words.add(\"'re\")\n",
    "    stop_words.add(\"''\")\n",
    "    stop_words.add('(')\n",
    "    stop_words.add('ca')\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    list_tokens=[]\n",
    "    for token in tokens:\n",
    "        list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    #Assign tag to token list to show what it is\n",
    "    test = nltk.pos_tag(list_tokens)\n",
    "    #Only use adjectives and verbs\n",
    "    final_tokens = [a[0] for a in test if (a[1] == 'JJ' or a[1] == 'VERB')]\n",
    "    return final_tokens\n",
    " \n",
    "# load doc and add to vocab\n",
    "def add(filename, vocab):\n",
    "    \n",
    "    tokens = clean_doc(filename)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    " \n",
    " \n",
    "#Vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "for instance in ([i[0] for i in train]):\n",
    "    add(instance, vocab)\n",
    "\n",
    "# Vocab size\n",
    "print(len(vocab))\n",
    "# Top 50 words\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(clean_doc,ngram_range=(1,2),max_features=1000)\n",
    "matrix = vectorizer.fit_transform([i[0] for i in train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train SVM model\n",
    "xtrain = matrix.toarray()\n",
    "ytrain = [i[1] for i in train]\n",
    "svm_clf = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf.fit(np.asarray(xtrain),np.asarray(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85      2442\n",
      "           1       0.86      0.85      0.85      2558\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.85      0.85      0.85      5000\n",
      "weighted avg       0.85      0.85      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xdev = vectorizer.transform([i[0] for i in dev]).toarray()\n",
    "predictions = svm_clf.predict(xdev)\n",
    "print(sklearn.metrics.classification_report(predictions,[i[1] for i in dev]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87      2452\n",
      "           1       0.88      0.87      0.87      2548\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(clean_doc,ngram_range=(1,3),max_features=2500)\n",
    "matrix = vectorizer2.fit_transform(np.asarray([i[0] for i in train]))\n",
    "xtrain = matrix.toarray()\n",
    "ytrain=[i[1] for i in train]\n",
    "\n",
    "svm_clf_nu=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf_nu.fit(np.asarray(xtrain),np.asarray(ytrain))\n",
    "\n",
    "xdev = vectorizer2.transform([i[0] for i in dev]).toarray()\n",
    "predictions = svm_clf_nu.predict(xdev)\n",
    "print(sklearn.metrics.classification_report(predictions,[i[1] for i in dev]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_f = TfidfVectorizer(clean_doc,ngram_range=(1,2),max_features=5000)\n",
    "xtrain = vectorizer_f.fit_transform([i[0] for i in train]).toarray()\n",
    "ytrain = np.asarray([i[1] for i in train])\n",
    "\n",
    "xFeatureBest = SelectKBest(chi2, k=500).fit(xtrain, ytrain)\n",
    "xtrainFeatureBest = xFeatureBest.transform(xtrain)\n",
    "\n",
    "xtest = vectorizer_f.transform([i[0] for i in test]).toarray()\n",
    "xtestFeatureBest =  xFeatureBest.transform(xtest)\n",
    "\n",
    "ytest = np.asarray([i[1] for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDl987FvH6IX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86      2375\n",
      "           1       0.89      0.85      0.87      2625\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_clf_3=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf_3.fit(np.asarray(xtrainFeatureBest),ytrain)\n",
    "predFeatureBest = svm_clf_3.predict(xtestFeatureBest)\n",
    "print(sklearn.metrics.classification_report(predFeatureBest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.869\n",
      "Recall: 0.868\n",
      "F1-Score: 0.868\n",
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
    "ytestGold = ytest\n",
    "ytextPred = predFeatureBest\n",
    "precision=precision_score(ytestGold, ytextPred, average='macro')\n",
    "recall=recall_score(ytestGold, ytextPred, average='macro')\n",
    "f1=f1_score(ytestGold, ytextPred, average='macro')\n",
    "accuracy=accuracy_score(ytestGold, ytextPred)\n",
    "\n",
    "print (\"Precision: \"+str(round(precision,3)))\n",
    "print (\"Recall: \"+str(round(recall,3)))\n",
    "print (\"F1-Score: \"+str(round(f1,3)))\n",
    "print (\"Accuracy: \"+str(round(accuracy,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2_FeatureEngineeringSelection_Sklearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
